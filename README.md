# TM2D
2D dimensional blocking algorithm (see Rohrer et al., submitted to Journal of Climate). Computed blocking catalogues for a multitude of dataset are available on request (marco.rohrer@giub.unibe.ch). 

The blocking algorithm combines the approach of Scherrer et al. (2006), who uses a two-dimensional extension of the original Tibaldi and Molteni (1990) algorithm based on the detection of reversals of the meridional 500 hPa geopotential heigt gradient. Following two criteria are required to flag a grid point as potentially blocked:
a) GPH gradient towards the pole: 		  GPHG_P=  (Z500_(φ+dφ) - Z500_(φ) )/dφ < -10 gpm/(°lat)
b) GPH gradient towards the equator:		GPHG_E=  (Z500_(φ) - Z500_(φ-dφ))/dφ  > 0  gpm/(°lat)
The difference between the central and the polarward or equatorward grid point dφ is given by Int((15°)/dy)*dy (where dy the horizontal resolution in latitudinal direction. For better comparability, all datasets are bilinearly remapped to a 2°×2° resolution. Therefore dy=Int((15°)/(2°))*2°= 14° and the latitude φ varies from 36° to 76° in 2° intervals. All datasets are also available in their original resolution, where dφ is adapted accordingly. 
Flagged grid points are thereafter checked according to an approach similar to Schwierz et al. (2004). Blockings are defined as spatio-temporally connected anomalies, where the overlap between two time steps needs to be larger than 0.7 roach of Schwierz et al. (2004), who defined blockings as a spatio-temporally connected anomaly. A blocking is detected, if the spatial overlap of a reversed GPHG area between two time steps is larger than 0.7 (i.e. A_(t) ⋂A_(t+1)  ≥0.7*A_(t), where At denotes the area of a blocking at time step t) and if the GPHG reversal persists at least five days (20 time steps).
On a more technical note, the algorithm was programmed in a way to detect blockings also on very long datasets such as CCC400 with 400 years of data. One large NetCDF file is created that is loaded sequentially into the blocking algorithm. This approaches enables to run the software with a relatively low RAM footprint, although very large input files (e.g. a complete CCC400 member is 40 GB large). Instead of loading the whole dataset at once, only approximately one year is loaded into the algorithm, reducing the memory footprint by a factor of roughly 400. As soon as a time step is not used anymore, as blockings can only be detected forward in time but not backward, the next time step is loaded until the end of the file is reached. This approach ensures that every detected blocking has an unique ID and evades problems arising if the datasets is examined in e.g. yearly chunks (i.e. time overlap needed to detect blockings at the beginning and end of the chunk). 
Drawback of the geopotential height definition is that Omega blocks (such as during the 2003 Central European heatwave) are not necessarily detected as long as there is no reversal of the gradient but only a poleward migration of a strong, quasi-stationary high pressure system. 
